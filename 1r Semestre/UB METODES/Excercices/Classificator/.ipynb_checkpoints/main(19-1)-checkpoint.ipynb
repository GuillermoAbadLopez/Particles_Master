{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebbaa87f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "be39e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import everything we need\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from datetime import datetime\n",
    "from scipy.io import arff\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90790304",
   "metadata": {},
   "source": [
    "# IMPORT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bcaa140e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x-box</th>\n",
       "      <th>y-box</th>\n",
       "      <th>width-box</th>\n",
       "      <th>heigth-box</th>\n",
       "      <th>npix</th>\n",
       "      <th>x-var-pix</th>\n",
       "      <th>y-var-pix</th>\n",
       "      <th>x2bar</th>\n",
       "      <th>y2bar</th>\n",
       "      <th>xybar</th>\n",
       "      <th>x2ybr</th>\n",
       "      <th>xy2br</th>\n",
       "      <th>x-ege</th>\n",
       "      <th>xegvy</th>\n",
       "      <th>y-ege</th>\n",
       "      <th>yegvx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       x-box  y-box  width-box  heigth-box  npix  x-var-pix  y-var-pix  x2bar  \\\n",
       "0        2.0    8.0        3.0         5.0   1.0        8.0       13.0    0.0   \n",
       "1        5.0   12.0        3.0         7.0   2.0       10.0        5.0    5.0   \n",
       "2        4.0   11.0        6.0         8.0   6.0       10.0        6.0    2.0   \n",
       "3        7.0   11.0        6.0         6.0   3.0        5.0        9.0    4.0   \n",
       "4        2.0    1.0        3.0         1.0   1.0        8.0        6.0    6.0   \n",
       "...      ...    ...        ...         ...   ...        ...        ...    ...   \n",
       "14995    4.0    9.0        6.0         7.0   4.0        7.0        7.0    8.0   \n",
       "14996    1.0    9.0        0.0         6.0   0.0        7.0        7.0    4.0   \n",
       "14997    6.0    9.0        9.0         7.0   5.0        3.0        9.0    3.0   \n",
       "14998    6.0    9.0        7.0         8.0   7.0        6.0        8.0    4.0   \n",
       "14999    6.0   11.0        6.0         8.0   3.0        4.0       13.0    9.0   \n",
       "\n",
       "       y2bar  xybar  x2ybr  xy2br  x-ege  xegvy  y-ege  yegvx  \n",
       "0        6.0    6.0   10.0    8.0    0.0    8.0    0.0    8.0  \n",
       "1        4.0   13.0    3.0    9.0    2.0    8.0    4.0   10.0  \n",
       "2        6.0   10.0    3.0    7.0    3.0    7.0    3.0    9.0  \n",
       "3        6.0    4.0    4.0   10.0    6.0   10.0    2.0    8.0  \n",
       "4        6.0    6.0    5.0    9.0    1.0    7.0    5.0   10.0  \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "14995    4.0    7.0    6.0   11.0    3.0    8.0    4.0    7.0  \n",
       "14996    4.0    7.0    6.0    8.0    0.0    8.0    0.0    8.0  \n",
       "14997    7.0   11.0   11.0   11.0    3.0    8.0    3.0    5.0  \n",
       "14998    9.0    8.0    8.0    8.0    3.0   10.0    8.0    7.0  \n",
       "14999    2.0   10.0    6.0    3.0    1.0   10.0    4.0    8.0  \n",
       "\n",
       "[15000 rows x 16 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = arff.loadarff('letter-recognition.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "df['class'] = df['class'].apply(lambda x : ord(x)-65)\n",
    "\n",
    "Ntraining=15000 #Total is 20k\n",
    "\n",
    "training_images=df.drop(columns='class')[:Ntraining]\n",
    "testing_images=df.drop(columns='class')[Ntraining:]\n",
    "\n",
    "training_labels=df['class'][:Ntraining]\n",
    "testing_labels=df['class'][Ntraining:]\n",
    "\n",
    "training_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0667a1",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d90a4b4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 2.1253 - accuracy: 0.3409 - val_loss: 1.5900 - val_accuracy: 0.4906\n",
      "Epoch 2/100\n",
      "469/469 [==============================] - 0s 980us/step - loss: 1.3890 - accuracy: 0.5600 - val_loss: 1.2874 - val_accuracy: 0.5806\n",
      "Epoch 3/100\n",
      "469/469 [==============================] - 0s 966us/step - loss: 1.1574 - accuracy: 0.6285 - val_loss: 1.1317 - val_accuracy: 0.6170\n",
      "Epoch 4/100\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 1.0207 - accuracy: 0.6715 - val_loss: 1.0280 - val_accuracy: 0.6766\n",
      "Epoch 5/100\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.8932 - accuracy: 0.7165 - val_loss: 0.9191 - val_accuracy: 0.7186\n",
      "Epoch 6/100\n",
      "469/469 [==============================] - 0s 923us/step - loss: 0.8236 - accuracy: 0.7410 - val_loss: 0.8541 - val_accuracy: 0.7356\n",
      "Epoch 7/100\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.7513 - accuracy: 0.7625 - val_loss: 0.8065 - val_accuracy: 0.7480\n",
      "Epoch 8/100\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.6982 - accuracy: 0.7781 - val_loss: 0.7164 - val_accuracy: 0.7800\n",
      "Epoch 9/100\n",
      "469/469 [==============================] - 0s 987us/step - loss: 0.6511 - accuracy: 0.7903 - val_loss: 0.6712 - val_accuracy: 0.7898\n",
      "Epoch 10/100\n",
      "469/469 [==============================] - 0s 964us/step - loss: 0.6186 - accuracy: 0.8013 - val_loss: 0.6416 - val_accuracy: 0.8004\n",
      "Epoch 11/100\n",
      "469/469 [==============================] - 0s 979us/step - loss: 0.5743 - accuracy: 0.8159 - val_loss: 0.6050 - val_accuracy: 0.8072\n",
      "Epoch 12/100\n",
      "469/469 [==============================] - 0s 983us/step - loss: 0.5456 - accuracy: 0.8243 - val_loss: 0.6642 - val_accuracy: 0.7896\n",
      "Epoch 13/100\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.5221 - accuracy: 0.8315 - val_loss: 0.5461 - val_accuracy: 0.8324\n",
      "Epoch 14/100\n",
      "469/469 [==============================] - 0s 985us/step - loss: 0.4993 - accuracy: 0.8405 - val_loss: 0.5272 - val_accuracy: 0.8370\n",
      "Epoch 15/100\n",
      "469/469 [==============================] - 0s 977us/step - loss: 0.4824 - accuracy: 0.8431 - val_loss: 0.5181 - val_accuracy: 0.8378\n",
      "Epoch 16/100\n",
      "469/469 [==============================] - 0s 990us/step - loss: 0.4561 - accuracy: 0.8518 - val_loss: 0.5400 - val_accuracy: 0.8346\n",
      "Epoch 17/100\n",
      "469/469 [==============================] - 0s 981us/step - loss: 0.4266 - accuracy: 0.8582 - val_loss: 0.5032 - val_accuracy: 0.8436\n",
      "Epoch 18/100\n",
      "469/469 [==============================] - 0s 971us/step - loss: 0.4124 - accuracy: 0.8645 - val_loss: 0.4557 - val_accuracy: 0.8556\n",
      "Epoch 19/100\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.4139 - accuracy: 0.8647 - val_loss: 0.4817 - val_accuracy: 0.8546\n",
      "Epoch 20/100\n",
      "469/469 [==============================] - 0s 972us/step - loss: 0.3908 - accuracy: 0.8705 - val_loss: 0.4759 - val_accuracy: 0.8494\n",
      "Epoch 21/100\n",
      "469/469 [==============================] - 0s 975us/step - loss: 0.3696 - accuracy: 0.8779 - val_loss: 0.4472 - val_accuracy: 0.8612\n",
      "Epoch 22/100\n",
      "469/469 [==============================] - 0s 947us/step - loss: 0.3618 - accuracy: 0.8767 - val_loss: 0.5341 - val_accuracy: 0.8270\n",
      "Epoch 23/100\n",
      "469/469 [==============================] - 0s 941us/step - loss: 0.3585 - accuracy: 0.8813 - val_loss: 0.4130 - val_accuracy: 0.8724\n",
      "Epoch 24/100\n",
      "469/469 [==============================] - 0s 938us/step - loss: 0.3436 - accuracy: 0.8863 - val_loss: 0.4326 - val_accuracy: 0.8692\n",
      "Epoch 25/100\n",
      "469/469 [==============================] - 0s 956us/step - loss: 0.3280 - accuracy: 0.8931 - val_loss: 0.4340 - val_accuracy: 0.8692\n",
      "Epoch 26/100\n",
      "469/469 [==============================] - 0s 960us/step - loss: 0.3206 - accuracy: 0.8933 - val_loss: 0.3995 - val_accuracy: 0.8742\n",
      "Epoch 27/100\n",
      "469/469 [==============================] - 0s 951us/step - loss: 0.3114 - accuracy: 0.8947 - val_loss: 0.4127 - val_accuracy: 0.8768\n",
      "Epoch 28/100\n",
      "469/469 [==============================] - 0s 937us/step - loss: 0.2878 - accuracy: 0.9037 - val_loss: 0.4244 - val_accuracy: 0.8724\n",
      "Epoch 29/100\n",
      "469/469 [==============================] - 0s 945us/step - loss: 0.3060 - accuracy: 0.8978 - val_loss: 0.3775 - val_accuracy: 0.8810\n",
      "Epoch 30/100\n",
      "469/469 [==============================] - 0s 938us/step - loss: 0.2866 - accuracy: 0.9051 - val_loss: 0.3603 - val_accuracy: 0.8848\n",
      "Epoch 31/100\n",
      "469/469 [==============================] - 0s 942us/step - loss: 0.2813 - accuracy: 0.9082 - val_loss: 0.3740 - val_accuracy: 0.8838\n",
      "Epoch 32/100\n",
      "469/469 [==============================] - 0s 957us/step - loss: 0.2842 - accuracy: 0.9056 - val_loss: 0.3464 - val_accuracy: 0.8940\n",
      "Epoch 33/100\n",
      "469/469 [==============================] - 0s 943us/step - loss: 0.2646 - accuracy: 0.9105 - val_loss: 0.3535 - val_accuracy: 0.8946\n",
      "Epoch 34/100\n",
      "469/469 [==============================] - 0s 946us/step - loss: 0.2680 - accuracy: 0.9094 - val_loss: 0.3571 - val_accuracy: 0.8934\n",
      "Epoch 35/100\n",
      "469/469 [==============================] - 0s 948us/step - loss: 0.2622 - accuracy: 0.9135 - val_loss: 0.3598 - val_accuracy: 0.8894\n",
      "Epoch 36/100\n",
      "469/469 [==============================] - 0s 943us/step - loss: 0.2513 - accuracy: 0.9154 - val_loss: 0.3596 - val_accuracy: 0.8888\n",
      "Epoch 37/100\n",
      "469/469 [==============================] - 0s 943us/step - loss: 0.2551 - accuracy: 0.9152 - val_loss: 0.3712 - val_accuracy: 0.8888\n",
      "Epoch 38/100\n",
      "469/469 [==============================] - 0s 966us/step - loss: 0.2388 - accuracy: 0.9199 - val_loss: 0.3783 - val_accuracy: 0.8896\n",
      "Epoch 39/100\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.2297 - accuracy: 0.9233 - val_loss: 0.3557 - val_accuracy: 0.8924\n",
      "Epoch 40/100\n",
      "469/469 [==============================] - 0s 992us/step - loss: 0.2266 - accuracy: 0.9223 - val_loss: 0.4035 - val_accuracy: 0.8898\n",
      "Epoch 41/100\n",
      "469/469 [==============================] - 0s 977us/step - loss: 0.2372 - accuracy: 0.9197 - val_loss: 0.3425 - val_accuracy: 0.9070\n",
      "Epoch 42/100\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.2263 - accuracy: 0.9249 - val_loss: 0.3324 - val_accuracy: 0.9018\n",
      "Epoch 43/100\n",
      "469/469 [==============================] - 0s 981us/step - loss: 0.2309 - accuracy: 0.9232 - val_loss: 0.3140 - val_accuracy: 0.9070\n",
      "Epoch 44/100\n",
      "469/469 [==============================] - 0s 970us/step - loss: 0.2059 - accuracy: 0.9287 - val_loss: 0.3528 - val_accuracy: 0.8954\n",
      "Epoch 45/100\n",
      "469/469 [==============================] - 0s 972us/step - loss: 0.2093 - accuracy: 0.9294 - val_loss: 0.3221 - val_accuracy: 0.9044\n",
      "Epoch 46/100\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.2136 - accuracy: 0.9265 - val_loss: 0.3076 - val_accuracy: 0.9098\n",
      "Epoch 47/100\n",
      "469/469 [==============================] - 0s 987us/step - loss: 0.2065 - accuracy: 0.9305 - val_loss: 0.3313 - val_accuracy: 0.9044\n",
      "Epoch 48/100\n",
      "469/469 [==============================] - 0s 962us/step - loss: 0.1934 - accuracy: 0.9333 - val_loss: 0.2940 - val_accuracy: 0.9136\n",
      "Epoch 49/100\n",
      "469/469 [==============================] - 0s 986us/step - loss: 0.1974 - accuracy: 0.9325 - val_loss: 0.3299 - val_accuracy: 0.9046\n",
      "Epoch 50/100\n",
      "469/469 [==============================] - 0s 951us/step - loss: 0.1975 - accuracy: 0.9314 - val_loss: 0.3616 - val_accuracy: 0.8926\n",
      "Epoch 51/100\n",
      "469/469 [==============================] - 0s 968us/step - loss: 0.1994 - accuracy: 0.9343 - val_loss: 0.3476 - val_accuracy: 0.9040\n",
      "Epoch 52/100\n",
      "469/469 [==============================] - 0s 941us/step - loss: 0.1980 - accuracy: 0.9333 - val_loss: 0.3174 - val_accuracy: 0.9058\n",
      "Epoch 53/100\n",
      "469/469 [==============================] - 0s 930us/step - loss: 0.1752 - accuracy: 0.9413 - val_loss: 0.3176 - val_accuracy: 0.9048\n",
      "Epoch 54/100\n",
      "469/469 [==============================] - 0s 953us/step - loss: 0.1797 - accuracy: 0.9383 - val_loss: 0.3162 - val_accuracy: 0.9154\n",
      "Epoch 55/100\n",
      "469/469 [==============================] - 0s 977us/step - loss: 0.1867 - accuracy: 0.9359 - val_loss: 0.3510 - val_accuracy: 0.9034\n",
      "Epoch 56/100\n",
      "469/469 [==============================] - 0s 981us/step - loss: 0.1767 - accuracy: 0.9400 - val_loss: 0.3238 - val_accuracy: 0.9096\n",
      "Epoch 57/100\n",
      "469/469 [==============================] - 0s 953us/step - loss: 0.1647 - accuracy: 0.9454 - val_loss: 0.3423 - val_accuracy: 0.8988\n",
      "Epoch 58/100\n",
      "469/469 [==============================] - 0s 966us/step - loss: 0.1851 - accuracy: 0.9345 - val_loss: 0.2979 - val_accuracy: 0.9130\n",
      "Epoch 59/100\n",
      "469/469 [==============================] - 0s 994us/step - loss: 0.1613 - accuracy: 0.9461 - val_loss: 0.2684 - val_accuracy: 0.9230\n",
      "Epoch 60/100\n",
      "469/469 [==============================] - 0s 966us/step - loss: 0.1668 - accuracy: 0.9442 - val_loss: 0.3145 - val_accuracy: 0.9100\n",
      "Epoch 61/100\n",
      "469/469 [==============================] - 0s 943us/step - loss: 0.1689 - accuracy: 0.9438 - val_loss: 0.3132 - val_accuracy: 0.9082\n",
      "Epoch 62/100\n",
      "469/469 [==============================] - 0s 945us/step - loss: 0.1546 - accuracy: 0.9462 - val_loss: 0.3444 - val_accuracy: 0.9090\n",
      "Epoch 63/100\n",
      "469/469 [==============================] - 0s 951us/step - loss: 0.1720 - accuracy: 0.9419 - val_loss: 0.2835 - val_accuracy: 0.9188\n",
      "Epoch 64/100\n",
      "469/469 [==============================] - 0s 955us/step - loss: 0.1628 - accuracy: 0.9430 - val_loss: 0.3271 - val_accuracy: 0.9098\n",
      "Epoch 65/100\n",
      "469/469 [==============================] - 0s 949us/step - loss: 0.1416 - accuracy: 0.9500 - val_loss: 0.3234 - val_accuracy: 0.9090\n",
      "Epoch 66/100\n",
      "469/469 [==============================] - 0s 970us/step - loss: 0.1536 - accuracy: 0.9467 - val_loss: 0.2862 - val_accuracy: 0.9226\n",
      "Epoch 67/100\n",
      "469/469 [==============================] - 0s 951us/step - loss: 0.1639 - accuracy: 0.9445 - val_loss: 0.3031 - val_accuracy: 0.9176\n",
      "Epoch 68/100\n",
      "469/469 [==============================] - 0s 968us/step - loss: 0.1372 - accuracy: 0.9536 - val_loss: 0.3662 - val_accuracy: 0.9084\n",
      "Epoch 69/100\n",
      "469/469 [==============================] - 0s 958us/step - loss: 0.1542 - accuracy: 0.9465 - val_loss: 0.3213 - val_accuracy: 0.9148\n",
      "Epoch 70/100\n",
      "469/469 [==============================] - 0s 957us/step - loss: 0.1351 - accuracy: 0.9525 - val_loss: 0.2688 - val_accuracy: 0.9250\n",
      "Epoch 71/100\n",
      "469/469 [==============================] - 0s 945us/step - loss: 0.1522 - accuracy: 0.9505 - val_loss: 0.2772 - val_accuracy: 0.9216\n",
      "Epoch 72/100\n",
      "469/469 [==============================] - 0s 964us/step - loss: 0.1391 - accuracy: 0.9525 - val_loss: 0.3530 - val_accuracy: 0.9072\n",
      "Epoch 73/100\n",
      "469/469 [==============================] - 0s 985us/step - loss: 0.1620 - accuracy: 0.9456 - val_loss: 0.3024 - val_accuracy: 0.9176\n",
      "Epoch 74/100\n",
      "469/469 [==============================] - 0s 998us/step - loss: 0.1306 - accuracy: 0.9557 - val_loss: 0.2820 - val_accuracy: 0.9238\n",
      "Epoch 75/100\n",
      "469/469 [==============================] - 0s 981us/step - loss: 0.1315 - accuracy: 0.9543 - val_loss: 0.3002 - val_accuracy: 0.9212\n",
      "Epoch 76/100\n",
      "469/469 [==============================] - 0s 994us/step - loss: 0.1424 - accuracy: 0.9529 - val_loss: 0.2809 - val_accuracy: 0.9216\n",
      "Epoch 77/100\n",
      "469/469 [==============================] - 0s 965us/step - loss: 0.1378 - accuracy: 0.9538 - val_loss: 0.3105 - val_accuracy: 0.9200\n",
      "Epoch 78/100\n",
      "469/469 [==============================] - 0s 972us/step - loss: 0.1192 - accuracy: 0.9594 - val_loss: 0.3156 - val_accuracy: 0.9152\n",
      "Epoch 79/100\n",
      "469/469 [==============================] - 0s 975us/step - loss: 0.1356 - accuracy: 0.9548 - val_loss: 0.3074 - val_accuracy: 0.9156\n",
      "Epoch 80/100\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9517 - val_loss: 0.2703 - val_accuracy: 0.9262\n",
      "Epoch 81/100\n",
      "469/469 [==============================] - 0s 977us/step - loss: 0.1081 - accuracy: 0.9622 - val_loss: 0.3086 - val_accuracy: 0.9228\n",
      "Epoch 82/100\n",
      "469/469 [==============================] - 0s 962us/step - loss: 0.1346 - accuracy: 0.9550 - val_loss: 0.3127 - val_accuracy: 0.9204\n",
      "Epoch 83/100\n",
      "469/469 [==============================] - 0s 947us/step - loss: 0.1282 - accuracy: 0.9561 - val_loss: 0.2756 - val_accuracy: 0.9264\n",
      "Epoch 84/100\n",
      "469/469 [==============================] - 0s 942us/step - loss: 0.1234 - accuracy: 0.9589 - val_loss: 0.3267 - val_accuracy: 0.9148\n",
      "Epoch 85/100\n",
      "469/469 [==============================] - 0s 953us/step - loss: 0.1251 - accuracy: 0.9591 - val_loss: 0.3066 - val_accuracy: 0.9154\n",
      "Epoch 86/100\n",
      "469/469 [==============================] - 0s 960us/step - loss: 0.1261 - accuracy: 0.9577 - val_loss: 0.3824 - val_accuracy: 0.9094\n",
      "Epoch 87/100\n",
      "469/469 [==============================] - 0s 979us/step - loss: 0.1080 - accuracy: 0.9614 - val_loss: 0.3186 - val_accuracy: 0.9150\n",
      "Epoch 88/100\n",
      "469/469 [==============================] - 0s 958us/step - loss: 0.1135 - accuracy: 0.9601 - val_loss: 0.3106 - val_accuracy: 0.9186\n",
      "Epoch 89/100\n",
      "469/469 [==============================] - 0s 944us/step - loss: 0.1174 - accuracy: 0.9589 - val_loss: 0.3002 - val_accuracy: 0.9222\n",
      "Epoch 90/100\n",
      "469/469 [==============================] - 0s 953us/step - loss: 0.1085 - accuracy: 0.9623 - val_loss: 0.3036 - val_accuracy: 0.9240\n",
      "Epoch 91/100\n",
      "469/469 [==============================] - 0s 996us/step - loss: 0.1240 - accuracy: 0.9577 - val_loss: 0.3142 - val_accuracy: 0.9232\n",
      "Epoch 92/100\n",
      "469/469 [==============================] - 0s 981us/step - loss: 0.1011 - accuracy: 0.9638 - val_loss: 0.3366 - val_accuracy: 0.9192\n",
      "Epoch 93/100\n",
      "469/469 [==============================] - 0s 990us/step - loss: 0.1315 - accuracy: 0.9555 - val_loss: 0.2986 - val_accuracy: 0.9224\n",
      "Epoch 94/100\n",
      "469/469 [==============================] - 0s 966us/step - loss: 0.1299 - accuracy: 0.9571 - val_loss: 0.2769 - val_accuracy: 0.9272\n",
      "Epoch 95/100\n",
      "469/469 [==============================] - 0s 973us/step - loss: 0.0973 - accuracy: 0.9670 - val_loss: 0.2661 - val_accuracy: 0.9354\n",
      "Epoch 96/100\n",
      "469/469 [==============================] - 0s 979us/step - loss: 0.1142 - accuracy: 0.9612 - val_loss: 0.3789 - val_accuracy: 0.9084\n",
      "Epoch 97/100\n",
      "469/469 [==============================] - 0s 994us/step - loss: 0.1122 - accuracy: 0.9619 - val_loss: 0.3041 - val_accuracy: 0.9226\n",
      "Epoch 98/100\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1137 - accuracy: 0.9608 - val_loss: 0.3406 - val_accuracy: 0.9216\n",
      "Epoch 99/100\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9654 - val_loss: 0.2511 - val_accuracy: 0.9334\n",
      "Epoch 100/100\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.1060 - accuracy: 0.9645 - val_loss: 0.2724 - val_accuracy: 0.9296\n",
      "157/157 [==============================] - 0s 522us/step - loss: 0.2724 - accuracy: 0.9296\n",
      "Loss: 0.2724216878414154\n",
      "Accuracy: 0.9296000003814697\n",
      "INFO:tensorflow:Assets written to: letter_classifier.model\\assets\n"
     ]
    }
   ],
   "source": [
    "#Create the NN (Convolutionals between MaxPoolings, Flatten, Dense layer and then get probabilities)\n",
    "model=models.Sequential()\n",
    "model.add(layers.Dense(16,activation='relu',input_shape = (16,)))\n",
    "model.add(layers.Dense(100,activation= 'relu'))\n",
    "model.add(layers.Dense(100,activation= 'relu'))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(100,activation= 'relu'))\n",
    "#Flatten the input for the result\n",
    "model.add(layers.Flatten())\n",
    "#Now a dense layer\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "#Finally let's get probabilities\n",
    "model.add(layers.Dense(26,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#We say how many times the NN will see the training data (the generations I supose)\n",
    "model.fit(training_images, training_labels, epochs=100, validation_data=(testing_images, testing_labels))\n",
    "\n",
    "loss, accuracy = model.evaluate(testing_images, testing_labels)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "model.save('letter_classifier.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "420e76ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.7027 - accuracy: 0.1511 - val_loss: 2.1049 - val_accuracy: 0.3296\n",
      "Epoch 2/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.7700 - accuracy: 0.4280 - val_loss: 1.5862 - val_accuracy: 0.5048\n",
      "Epoch 3/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.4042 - accuracy: 0.5539 - val_loss: 1.2848 - val_accuracy: 0.5806\n",
      "Epoch 4/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.1350 - accuracy: 0.6451 - val_loss: 1.0837 - val_accuracy: 0.6656\n",
      "Epoch 5/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.9884 - accuracy: 0.6927 - val_loss: 0.9830 - val_accuracy: 0.6938\n",
      "Epoch 6/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.8844 - accuracy: 0.7227 - val_loss: 0.9429 - val_accuracy: 0.6996\n",
      "Epoch 7/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.8046 - accuracy: 0.7491 - val_loss: 0.8038 - val_accuracy: 0.7546\n",
      "Epoch 8/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7324 - accuracy: 0.7735 - val_loss: 0.7794 - val_accuracy: 0.7588\n",
      "Epoch 9/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6714 - accuracy: 0.7865 - val_loss: 0.6904 - val_accuracy: 0.7860\n",
      "Epoch 10/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6184 - accuracy: 0.8043 - val_loss: 0.6832 - val_accuracy: 0.7860\n",
      "Epoch 11/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6050 - accuracy: 0.8045 - val_loss: 0.6005 - val_accuracy: 0.8096\n",
      "Epoch 12/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5545 - accuracy: 0.8211 - val_loss: 0.5947 - val_accuracy: 0.8178\n",
      "Epoch 13/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5208 - accuracy: 0.8355 - val_loss: 0.5908 - val_accuracy: 0.8098\n",
      "Epoch 14/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4976 - accuracy: 0.8393 - val_loss: 0.5894 - val_accuracy: 0.8186\n",
      "Epoch 15/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4721 - accuracy: 0.8464 - val_loss: 0.5394 - val_accuracy: 0.8380\n",
      "Epoch 16/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4585 - accuracy: 0.8503 - val_loss: 0.5271 - val_accuracy: 0.8324\n",
      "Epoch 17/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4319 - accuracy: 0.8585 - val_loss: 0.4987 - val_accuracy: 0.8422\n",
      "Epoch 18/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4114 - accuracy: 0.8648 - val_loss: 0.4723 - val_accuracy: 0.8466\n",
      "Epoch 19/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4032 - accuracy: 0.8670 - val_loss: 0.4837 - val_accuracy: 0.8470\n",
      "Epoch 20/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3843 - accuracy: 0.8711 - val_loss: 0.5283 - val_accuracy: 0.8386\n",
      "Epoch 21/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3697 - accuracy: 0.8796 - val_loss: 0.5507 - val_accuracy: 0.8322\n",
      "Epoch 22/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3586 - accuracy: 0.8804 - val_loss: 0.4173 - val_accuracy: 0.8706\n",
      "Epoch 23/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3385 - accuracy: 0.8883 - val_loss: 0.4866 - val_accuracy: 0.8434\n",
      "Epoch 24/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3302 - accuracy: 0.8899 - val_loss: 0.4313 - val_accuracy: 0.8712\n",
      "Epoch 25/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3155 - accuracy: 0.8953 - val_loss: 0.3957 - val_accuracy: 0.8742\n",
      "Epoch 26/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3019 - accuracy: 0.9005 - val_loss: 0.4131 - val_accuracy: 0.8754\n",
      "Epoch 27/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2916 - accuracy: 0.9027 - val_loss: 0.3818 - val_accuracy: 0.8828\n",
      "Epoch 28/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2884 - accuracy: 0.9055 - val_loss: 0.3671 - val_accuracy: 0.8826\n",
      "Epoch 29/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2791 - accuracy: 0.9091 - val_loss: 0.4457 - val_accuracy: 0.8694\n",
      "Epoch 30/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2768 - accuracy: 0.9111 - val_loss: 0.4216 - val_accuracy: 0.8708\n",
      "Epoch 31/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2724 - accuracy: 0.9107 - val_loss: 0.3743 - val_accuracy: 0.8872\n",
      "Epoch 32/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2494 - accuracy: 0.9171 - val_loss: 0.4473 - val_accuracy: 0.8634\n",
      "Epoch 33/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2431 - accuracy: 0.9215 - val_loss: 0.3900 - val_accuracy: 0.8852\n",
      "Epoch 34/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2431 - accuracy: 0.9203 - val_loss: 0.3377 - val_accuracy: 0.8996\n",
      "Epoch 35/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2448 - accuracy: 0.9207 - val_loss: 0.4119 - val_accuracy: 0.8806\n",
      "Epoch 36/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2306 - accuracy: 0.9253 - val_loss: 0.3423 - val_accuracy: 0.8948\n",
      "Epoch 37/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2258 - accuracy: 0.9253 - val_loss: 0.3937 - val_accuracy: 0.8828\n",
      "Epoch 38/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2192 - accuracy: 0.9281 - val_loss: 0.3579 - val_accuracy: 0.8924\n",
      "Epoch 39/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2136 - accuracy: 0.9281 - val_loss: 0.3850 - val_accuracy: 0.8888\n",
      "Epoch 40/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2150 - accuracy: 0.9293 - val_loss: 0.3042 - val_accuracy: 0.9110\n",
      "Epoch 41/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2102 - accuracy: 0.9316 - val_loss: 0.3244 - val_accuracy: 0.9018\n",
      "Epoch 42/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1996 - accuracy: 0.9341 - val_loss: 0.3103 - val_accuracy: 0.9096\n",
      "Epoch 43/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1873 - accuracy: 0.9381 - val_loss: 0.3007 - val_accuracy: 0.9102\n",
      "Epoch 44/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1932 - accuracy: 0.9375 - val_loss: 0.3518 - val_accuracy: 0.8986\n",
      "Epoch 45/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1873 - accuracy: 0.9385 - val_loss: 0.3551 - val_accuracy: 0.9044\n",
      "Epoch 46/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1781 - accuracy: 0.9421 - val_loss: 0.3248 - val_accuracy: 0.9054\n",
      "Epoch 47/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9393 - val_loss: 0.3301 - val_accuracy: 0.9020\n",
      "Epoch 48/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1933 - accuracy: 0.9362 - val_loss: 0.3067 - val_accuracy: 0.9118\n",
      "Epoch 49/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1833 - accuracy: 0.9393 - val_loss: 0.2942 - val_accuracy: 0.9152\n",
      "Epoch 50/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1648 - accuracy: 0.9459 - val_loss: 0.3424 - val_accuracy: 0.9088\n",
      "Epoch 51/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1766 - accuracy: 0.9415 - val_loss: 0.2954 - val_accuracy: 0.9162\n",
      "Epoch 52/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1665 - accuracy: 0.9455 - val_loss: 0.3349 - val_accuracy: 0.9052\n",
      "Epoch 53/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1779 - accuracy: 0.9434 - val_loss: 0.3142 - val_accuracy: 0.9080\n",
      "Epoch 54/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1595 - accuracy: 0.9476 - val_loss: 0.3490 - val_accuracy: 0.9108\n",
      "Epoch 55/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1802 - accuracy: 0.9415 - val_loss: 0.3104 - val_accuracy: 0.9134\n",
      "Epoch 56/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1567 - accuracy: 0.9480 - val_loss: 0.3371 - val_accuracy: 0.9054\n",
      "Epoch 57/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1465 - accuracy: 0.9511 - val_loss: 0.3404 - val_accuracy: 0.9064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1630 - accuracy: 0.9452 - val_loss: 0.3263 - val_accuracy: 0.9080\n",
      "Epoch 59/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1579 - accuracy: 0.9469 - val_loss: 0.3112 - val_accuracy: 0.9148\n",
      "Epoch 60/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1460 - accuracy: 0.9513 - val_loss: 0.2898 - val_accuracy: 0.9190\n",
      "Epoch 61/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1522 - accuracy: 0.9491 - val_loss: 0.3235 - val_accuracy: 0.9106\n",
      "Epoch 62/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1527 - accuracy: 0.9493 - val_loss: 0.3213 - val_accuracy: 0.9124\n",
      "Epoch 63/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1397 - accuracy: 0.9557 - val_loss: 0.3145 - val_accuracy: 0.9130\n",
      "Epoch 64/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1342 - accuracy: 0.9545 - val_loss: 0.3869 - val_accuracy: 0.9024\n",
      "Epoch 65/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1463 - accuracy: 0.9527 - val_loss: 0.3323 - val_accuracy: 0.9128\n",
      "Epoch 66/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1354 - accuracy: 0.9538 - val_loss: 0.2976 - val_accuracy: 0.9210\n",
      "Epoch 67/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1339 - accuracy: 0.9525 - val_loss: 0.3601 - val_accuracy: 0.9026\n",
      "Epoch 68/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1325 - accuracy: 0.9553 - val_loss: 0.2812 - val_accuracy: 0.9214\n",
      "Epoch 69/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1303 - accuracy: 0.9555 - val_loss: 0.3181 - val_accuracy: 0.9220\n",
      "Epoch 70/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1502 - accuracy: 0.9502 - val_loss: 0.3602 - val_accuracy: 0.9160\n",
      "Epoch 71/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1336 - accuracy: 0.9568 - val_loss: 0.3057 - val_accuracy: 0.9200\n",
      "Epoch 72/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1260 - accuracy: 0.9580 - val_loss: 0.3275 - val_accuracy: 0.9152\n",
      "Epoch 73/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1268 - accuracy: 0.9575 - val_loss: 0.3836 - val_accuracy: 0.8968\n",
      "Epoch 74/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1360 - accuracy: 0.9528 - val_loss: 0.3551 - val_accuracy: 0.9094\n",
      "Epoch 75/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1151 - accuracy: 0.9612 - val_loss: 0.3315 - val_accuracy: 0.9158\n",
      "Epoch 76/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1293 - accuracy: 0.9562 - val_loss: 0.2813 - val_accuracy: 0.9246\n",
      "Epoch 77/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1257 - accuracy: 0.9586 - val_loss: 0.3739 - val_accuracy: 0.9032\n",
      "Epoch 78/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1326 - accuracy: 0.9564 - val_loss: 0.3124 - val_accuracy: 0.9182\n",
      "Epoch 79/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1375 - accuracy: 0.9550 - val_loss: 0.2723 - val_accuracy: 0.9278\n",
      "Epoch 80/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1071 - accuracy: 0.9636 - val_loss: 0.2991 - val_accuracy: 0.9148\n",
      "Epoch 81/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1137 - accuracy: 0.9611 - val_loss: 0.2841 - val_accuracy: 0.9240\n",
      "Epoch 82/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1140 - accuracy: 0.9619 - val_loss: 0.3241 - val_accuracy: 0.9228\n",
      "Epoch 83/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1195 - accuracy: 0.9601 - val_loss: 0.2795 - val_accuracy: 0.9268\n",
      "Epoch 84/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1104 - accuracy: 0.9621 - val_loss: 0.3060 - val_accuracy: 0.9252\n",
      "Epoch 85/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1208 - accuracy: 0.9618 - val_loss: 0.3051 - val_accuracy: 0.9194\n",
      "Epoch 86/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1069 - accuracy: 0.9637 - val_loss: 0.3026 - val_accuracy: 0.9228\n",
      "Epoch 87/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1079 - accuracy: 0.9631 - val_loss: 0.3142 - val_accuracy: 0.9230\n",
      "Epoch 88/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1024 - accuracy: 0.9639 - val_loss: 0.3388 - val_accuracy: 0.9144\n",
      "Epoch 89/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1099 - accuracy: 0.9637 - val_loss: 0.3866 - val_accuracy: 0.8982\n",
      "Epoch 90/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1018 - accuracy: 0.9649 - val_loss: 0.2954 - val_accuracy: 0.9236\n",
      "Epoch 91/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1291 - accuracy: 0.9568 - val_loss: 0.2724 - val_accuracy: 0.9294\n",
      "Epoch 92/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0871 - accuracy: 0.9703 - val_loss: 0.2754 - val_accuracy: 0.9324\n",
      "Epoch 93/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1039 - accuracy: 0.9649 - val_loss: 0.3018 - val_accuracy: 0.9224\n",
      "Epoch 94/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1050 - accuracy: 0.9662 - val_loss: 0.3034 - val_accuracy: 0.9274\n",
      "Epoch 95/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1109 - accuracy: 0.9628 - val_loss: 0.3102 - val_accuracy: 0.9282\n",
      "Epoch 96/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0931 - accuracy: 0.9684 - val_loss: 0.2671 - val_accuracy: 0.9370\n",
      "Epoch 97/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1171 - accuracy: 0.9596 - val_loss: 0.2830 - val_accuracy: 0.9296\n",
      "Epoch 98/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1022 - accuracy: 0.9679 - val_loss: 0.2954 - val_accuracy: 0.9266\n",
      "Epoch 99/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0953 - accuracy: 0.9700 - val_loss: 0.3266 - val_accuracy: 0.9232\n",
      "Epoch 100/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1084 - accuracy: 0.9647 - val_loss: 0.2984 - val_accuracy: 0.9242\n",
      "Epoch 101/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0936 - accuracy: 0.9680 - val_loss: 0.2901 - val_accuracy: 0.9266\n",
      "Epoch 102/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0839 - accuracy: 0.9713 - val_loss: 0.2771 - val_accuracy: 0.9314\n",
      "Epoch 103/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1061 - accuracy: 0.9653 - val_loss: 0.3340 - val_accuracy: 0.9232\n",
      "Epoch 104/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0938 - accuracy: 0.9692 - val_loss: 0.3087 - val_accuracy: 0.9248\n",
      "Epoch 105/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0861 - accuracy: 0.9709 - val_loss: 0.3173 - val_accuracy: 0.9260\n",
      "Epoch 106/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1053 - accuracy: 0.9654 - val_loss: 0.3252 - val_accuracy: 0.9216\n",
      "Epoch 107/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0838 - accuracy: 0.9716 - val_loss: 0.2671 - val_accuracy: 0.9346\n",
      "Epoch 108/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0963 - accuracy: 0.9681 - val_loss: 0.3099 - val_accuracy: 0.9242\n",
      "Epoch 109/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0754 - accuracy: 0.9740 - val_loss: 0.3083 - val_accuracy: 0.9286\n",
      "Epoch 110/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1123 - accuracy: 0.9641 - val_loss: 0.3157 - val_accuracy: 0.9156\n",
      "Epoch 111/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0827 - accuracy: 0.9724 - val_loss: 0.2745 - val_accuracy: 0.9352\n",
      "Epoch 112/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0843 - accuracy: 0.9709 - val_loss: 0.3112 - val_accuracy: 0.9264\n",
      "Epoch 113/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0969 - accuracy: 0.9683 - val_loss: 0.2754 - val_accuracy: 0.9308\n",
      "Epoch 114/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0899 - accuracy: 0.9699 - val_loss: 0.3465 - val_accuracy: 0.9214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1001 - accuracy: 0.9678 - val_loss: 0.3062 - val_accuracy: 0.9292\n",
      "Epoch 116/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0686 - accuracy: 0.9770 - val_loss: 0.2824 - val_accuracy: 0.9292\n",
      "Epoch 117/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0930 - accuracy: 0.9687 - val_loss: 0.3323 - val_accuracy: 0.9220\n",
      "Epoch 118/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0859 - accuracy: 0.9708 - val_loss: 0.3183 - val_accuracy: 0.9244\n",
      "Epoch 119/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0765 - accuracy: 0.9747 - val_loss: 0.2697 - val_accuracy: 0.9322\n",
      "Epoch 120/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1034 - accuracy: 0.9685 - val_loss: 0.3170 - val_accuracy: 0.9270\n",
      "Epoch 121/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0662 - accuracy: 0.9789 - val_loss: 0.3130 - val_accuracy: 0.9266\n",
      "Epoch 122/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0900 - accuracy: 0.9711 - val_loss: 0.3481 - val_accuracy: 0.9252\n",
      "Epoch 123/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0826 - accuracy: 0.9728 - val_loss: 0.3566 - val_accuracy: 0.9226\n",
      "Epoch 124/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0795 - accuracy: 0.9743 - val_loss: 0.3216 - val_accuracy: 0.9300\n",
      "Epoch 125/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0906 - accuracy: 0.9699 - val_loss: 0.3039 - val_accuracy: 0.9328\n",
      "Epoch 126/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0651 - accuracy: 0.9777 - val_loss: 0.3372 - val_accuracy: 0.9254\n",
      "Epoch 127/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0987 - accuracy: 0.9677 - val_loss: 0.3295 - val_accuracy: 0.9274\n",
      "Epoch 128/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0685 - accuracy: 0.9758 - val_loss: 0.2922 - val_accuracy: 0.9322\n",
      "Epoch 129/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0913 - accuracy: 0.9716 - val_loss: 0.3401 - val_accuracy: 0.9198\n",
      "Epoch 130/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0847 - accuracy: 0.9721 - val_loss: 0.3106 - val_accuracy: 0.9310\n",
      "Epoch 131/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0624 - accuracy: 0.9787 - val_loss: 0.3628 - val_accuracy: 0.9196\n",
      "Epoch 132/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0768 - accuracy: 0.9750 - val_loss: 0.3859 - val_accuracy: 0.9174\n",
      "Epoch 133/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1021 - accuracy: 0.9684 - val_loss: 0.3115 - val_accuracy: 0.9290\n",
      "Epoch 134/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0593 - accuracy: 0.9795 - val_loss: 0.3164 - val_accuracy: 0.9294\n",
      "Epoch 135/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0704 - accuracy: 0.9764 - val_loss: 0.3399 - val_accuracy: 0.9256\n",
      "Epoch 136/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0912 - accuracy: 0.9707 - val_loss: 0.3529 - val_accuracy: 0.9260\n",
      "Epoch 137/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0917 - accuracy: 0.9715 - val_loss: 0.2760 - val_accuracy: 0.9334\n",
      "Epoch 138/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0751 - accuracy: 0.9743 - val_loss: 0.3308 - val_accuracy: 0.9230\n",
      "Epoch 139/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0634 - accuracy: 0.9777 - val_loss: 0.2925 - val_accuracy: 0.9356\n",
      "Epoch 140/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0774 - accuracy: 0.9752 - val_loss: 0.3278 - val_accuracy: 0.9280\n",
      "Epoch 141/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0793 - accuracy: 0.9742 - val_loss: 0.2988 - val_accuracy: 0.9358\n",
      "Epoch 142/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0749 - accuracy: 0.9762 - val_loss: 0.2790 - val_accuracy: 0.9388\n",
      "Epoch 143/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0623 - accuracy: 0.9784 - val_loss: 0.4363 - val_accuracy: 0.9126\n",
      "Epoch 144/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0835 - accuracy: 0.9723 - val_loss: 0.2883 - val_accuracy: 0.9356\n",
      "Epoch 145/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0737 - accuracy: 0.9741 - val_loss: 0.3940 - val_accuracy: 0.9150\n",
      "Epoch 146/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0958 - accuracy: 0.9697 - val_loss: 0.3316 - val_accuracy: 0.9310\n",
      "Epoch 147/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0562 - accuracy: 0.9809 - val_loss: 0.3434 - val_accuracy: 0.9312\n",
      "Epoch 148/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0868 - accuracy: 0.9729 - val_loss: 0.3072 - val_accuracy: 0.9336\n",
      "Epoch 149/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0852 - accuracy: 0.9717 - val_loss: 0.3221 - val_accuracy: 0.9282\n",
      "Epoch 150/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0622 - accuracy: 0.9804 - val_loss: 0.2975 - val_accuracy: 0.9320\n",
      "Epoch 151/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0525 - accuracy: 0.9825 - val_loss: 0.3296 - val_accuracy: 0.9304\n",
      "Epoch 152/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1200 - accuracy: 0.9665 - val_loss: 0.2755 - val_accuracy: 0.9322\n",
      "Epoch 153/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0539 - accuracy: 0.9821 - val_loss: 0.3199 - val_accuracy: 0.9314\n",
      "Epoch 154/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0626 - accuracy: 0.9802 - val_loss: 0.2741 - val_accuracy: 0.9384\n",
      "Epoch 155/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0771 - accuracy: 0.9725 - val_loss: 0.3249 - val_accuracy: 0.9300\n",
      "Epoch 156/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0924 - accuracy: 0.9691 - val_loss: 0.3014 - val_accuracy: 0.9362\n",
      "Epoch 157/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0601 - accuracy: 0.9789 - val_loss: 0.3923 - val_accuracy: 0.9158\n",
      "Epoch 158/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0725 - accuracy: 0.9747 - val_loss: 0.3079 - val_accuracy: 0.9336\n",
      "Epoch 159/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0759 - accuracy: 0.9763 - val_loss: 0.3520 - val_accuracy: 0.9292\n",
      "Epoch 160/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0802 - accuracy: 0.9739 - val_loss: 0.3038 - val_accuracy: 0.9354\n",
      "Epoch 161/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0861 - accuracy: 0.9726 - val_loss: 0.2722 - val_accuracy: 0.9390\n",
      "Epoch 162/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0536 - accuracy: 0.9829 - val_loss: 0.3407 - val_accuracy: 0.9296\n",
      "Epoch 163/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0611 - accuracy: 0.9791 - val_loss: 0.2766 - val_accuracy: 0.9380\n",
      "Epoch 164/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0970 - accuracy: 0.9694 - val_loss: 0.2762 - val_accuracy: 0.9396\n",
      "Epoch 165/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0497 - accuracy: 0.9825 - val_loss: 0.3071 - val_accuracy: 0.9382\n",
      "Epoch 166/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0589 - accuracy: 0.9797 - val_loss: 0.3060 - val_accuracy: 0.9332\n",
      "Epoch 167/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0730 - accuracy: 0.9758 - val_loss: 0.3144 - val_accuracy: 0.9356\n",
      "Epoch 168/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0650 - accuracy: 0.9781 - val_loss: 0.3548 - val_accuracy: 0.9264\n",
      "Epoch 169/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0845 - accuracy: 0.9731 - val_loss: 0.2920 - val_accuracy: 0.9362\n",
      "Epoch 170/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0593 - accuracy: 0.9805 - val_loss: 0.3119 - val_accuracy: 0.9326\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0817 - accuracy: 0.9733 - val_loss: 0.2874 - val_accuracy: 0.9362\n",
      "Epoch 172/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0521 - accuracy: 0.9820 - val_loss: 0.2850 - val_accuracy: 0.9394\n",
      "Epoch 173/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0565 - accuracy: 0.9809 - val_loss: 0.3946 - val_accuracy: 0.9234\n",
      "Epoch 174/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0651 - accuracy: 0.9787 - val_loss: 0.2921 - val_accuracy: 0.9354\n",
      "Epoch 175/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0894 - accuracy: 0.9721 - val_loss: 0.3181 - val_accuracy: 0.9290\n",
      "Epoch 176/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0748 - accuracy: 0.9768 - val_loss: 0.4363 - val_accuracy: 0.9064\n",
      "Epoch 177/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0663 - accuracy: 0.9787 - val_loss: 0.3360 - val_accuracy: 0.9304\n",
      "Epoch 178/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0388 - accuracy: 0.9874 - val_loss: 0.2917 - val_accuracy: 0.9400\n",
      "Epoch 179/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0810 - accuracy: 0.9749 - val_loss: 0.3496 - val_accuracy: 0.9292\n",
      "Epoch 180/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0653 - accuracy: 0.9781 - val_loss: 0.3429 - val_accuracy: 0.9346\n",
      "Epoch 181/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0557 - accuracy: 0.9809 - val_loss: 0.4527 - val_accuracy: 0.9138\n",
      "Epoch 182/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0776 - accuracy: 0.9739 - val_loss: 0.3189 - val_accuracy: 0.9334\n",
      "Epoch 183/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0450 - accuracy: 0.9846 - val_loss: 0.2678 - val_accuracy: 0.9400\n",
      "Epoch 184/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0605 - accuracy: 0.9797 - val_loss: 0.3478 - val_accuracy: 0.9312\n",
      "Epoch 185/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0736 - accuracy: 0.9750 - val_loss: 0.3117 - val_accuracy: 0.9384\n",
      "Epoch 186/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0668 - accuracy: 0.9775 - val_loss: 0.2726 - val_accuracy: 0.9374\n",
      "Epoch 187/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0681 - accuracy: 0.9783 - val_loss: 0.2900 - val_accuracy: 0.9392\n",
      "Epoch 188/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0613 - accuracy: 0.9802 - val_loss: 0.3224 - val_accuracy: 0.9334\n",
      "Epoch 189/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0499 - accuracy: 0.9832 - val_loss: 0.2757 - val_accuracy: 0.9420\n",
      "Epoch 190/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0552 - accuracy: 0.9834 - val_loss: 0.4578 - val_accuracy: 0.9138\n",
      "Epoch 191/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1087 - accuracy: 0.9686 - val_loss: 0.4208 - val_accuracy: 0.9102\n",
      "Epoch 192/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0555 - accuracy: 0.9832 - val_loss: 0.2525 - val_accuracy: 0.9444\n",
      "Epoch 193/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0296 - accuracy: 0.9903 - val_loss: 0.3200 - val_accuracy: 0.9384\n",
      "Epoch 194/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0580 - accuracy: 0.9805 - val_loss: 0.3860 - val_accuracy: 0.9240\n",
      "Epoch 195/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0663 - accuracy: 0.9771 - val_loss: 0.3087 - val_accuracy: 0.9336\n",
      "Epoch 196/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0486 - accuracy: 0.9846 - val_loss: 0.2931 - val_accuracy: 0.9396\n",
      "Epoch 197/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0500 - accuracy: 0.9836 - val_loss: 0.3225 - val_accuracy: 0.9350\n",
      "Epoch 198/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0933 - accuracy: 0.9731 - val_loss: 0.3449 - val_accuracy: 0.9184\n",
      "Epoch 199/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0446 - accuracy: 0.9851 - val_loss: 0.2978 - val_accuracy: 0.9376\n",
      "Epoch 200/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0421 - accuracy: 0.9853 - val_loss: 0.2755 - val_accuracy: 0.9444\n",
      "157/157 [==============================] - 0s 1ms/step - loss: 0.2755 - accuracy: 0.9444\n",
      "Loss: 0.27552923560142517\n",
      "Accuracy: 0.9444000124931335\n",
      "INFO:tensorflow:Assets written to: letter_classifier3.model\\assets\n"
     ]
    }
   ],
   "source": [
    "#Create the NN (Convolutionals between MaxPoolings, Flatten, Dense layer and then get probabilities)\n",
    "model=models.Sequential()\n",
    "model.add(layers.Dense(16,activation='relu',input_shape = (16,)))\n",
    "model.add(layers.Dense(100,activation= 'relu'))\n",
    "model.add(layers.Dense(100,activation= 'relu'))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(100,activation= 'relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "#Finally let's get probabilities\n",
    "model.add(layers.Dense(26,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#We say how many times the NN will see the training data (the generations I supose)\n",
    "model.fit(training_images, training_labels, epochs=200, validation_data=(testing_images, testing_labels))\n",
    "\n",
    "loss, accuracy = model.evaluate(testing_images, testing_labels)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "model.save('letter_classifier3.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "60130e55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 2.6404 - accuracy: 0.1891 - val_loss: 2.1006 - val_accuracy: 0.3238\n",
      "Epoch 2/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.8122 - accuracy: 0.4240 - val_loss: 1.6201 - val_accuracy: 0.5108\n",
      "Epoch 3/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.5172 - accuracy: 0.5351 - val_loss: 1.4467 - val_accuracy: 0.5576\n",
      "Epoch 4/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.3568 - accuracy: 0.5797 - val_loss: 1.3576 - val_accuracy: 0.5712\n",
      "Epoch 5/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.2554 - accuracy: 0.6053 - val_loss: 1.2490 - val_accuracy: 0.5990\n",
      "Epoch 6/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.1762 - accuracy: 0.6212 - val_loss: 1.2551 - val_accuracy: 0.6018\n",
      "Epoch 7/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.1271 - accuracy: 0.6408 - val_loss: 1.1415 - val_accuracy: 0.6362\n",
      "Epoch 8/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.0703 - accuracy: 0.6598 - val_loss: 1.1317 - val_accuracy: 0.6340\n",
      "Epoch 9/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.0362 - accuracy: 0.6647 - val_loss: 1.1158 - val_accuracy: 0.6382\n",
      "Epoch 10/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.9993 - accuracy: 0.6855 - val_loss: 1.0069 - val_accuracy: 0.6832\n",
      "Epoch 11/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.9640 - accuracy: 0.6929 - val_loss: 1.0420 - val_accuracy: 0.6660\n",
      "Epoch 12/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.9238 - accuracy: 0.7035 - val_loss: 1.0243 - val_accuracy: 0.6744\n",
      "Epoch 13/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.8937 - accuracy: 0.7151 - val_loss: 0.9361 - val_accuracy: 0.7156\n",
      "Epoch 14/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.8743 - accuracy: 0.7265 - val_loss: 0.9885 - val_accuracy: 0.6846\n",
      "Epoch 15/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.8394 - accuracy: 0.7321 - val_loss: 0.9538 - val_accuracy: 0.6966\n",
      "Epoch 16/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.8219 - accuracy: 0.7391 - val_loss: 0.9123 - val_accuracy: 0.7144\n",
      "Epoch 17/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.8014 - accuracy: 0.7423 - val_loss: 0.8460 - val_accuracy: 0.7356\n",
      "Epoch 18/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7675 - accuracy: 0.7550 - val_loss: 0.8547 - val_accuracy: 0.7358\n",
      "Epoch 19/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7564 - accuracy: 0.7563 - val_loss: 0.8088 - val_accuracy: 0.7426\n",
      "Epoch 20/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7329 - accuracy: 0.7650 - val_loss: 0.7789 - val_accuracy: 0.7566\n",
      "Epoch 21/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7139 - accuracy: 0.7690 - val_loss: 0.8593 - val_accuracy: 0.7290\n",
      "Epoch 22/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6960 - accuracy: 0.7766 - val_loss: 0.7719 - val_accuracy: 0.7640\n",
      "Epoch 23/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6710 - accuracy: 0.7845 - val_loss: 0.7474 - val_accuracy: 0.7656\n",
      "Epoch 24/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6489 - accuracy: 0.7923 - val_loss: 0.7118 - val_accuracy: 0.7742\n",
      "Epoch 25/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6511 - accuracy: 0.7898 - val_loss: 0.7255 - val_accuracy: 0.7746\n",
      "Epoch 26/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6291 - accuracy: 0.7956 - val_loss: 0.6515 - val_accuracy: 0.8002\n",
      "Epoch 27/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5990 - accuracy: 0.8053 - val_loss: 0.7350 - val_accuracy: 0.7760\n",
      "Epoch 28/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5919 - accuracy: 0.8105 - val_loss: 0.6377 - val_accuracy: 0.8066\n",
      "Epoch 29/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5784 - accuracy: 0.8138 - val_loss: 0.6645 - val_accuracy: 0.7978\n",
      "Epoch 30/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5739 - accuracy: 0.8185 - val_loss: 0.6321 - val_accuracy: 0.8092\n",
      "Epoch 31/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5629 - accuracy: 0.8199 - val_loss: 0.6965 - val_accuracy: 0.7786\n",
      "Epoch 32/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5530 - accuracy: 0.8233 - val_loss: 0.6211 - val_accuracy: 0.8080\n",
      "Epoch 33/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5359 - accuracy: 0.8294 - val_loss: 0.5975 - val_accuracy: 0.8148\n",
      "Epoch 34/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5312 - accuracy: 0.8264 - val_loss: 0.5898 - val_accuracy: 0.8204\n",
      "Epoch 35/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5227 - accuracy: 0.8325 - val_loss: 0.5785 - val_accuracy: 0.8270\n",
      "Epoch 36/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5057 - accuracy: 0.8384 - val_loss: 0.5828 - val_accuracy: 0.8230\n",
      "Epoch 37/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5021 - accuracy: 0.8405 - val_loss: 0.6162 - val_accuracy: 0.8134\n",
      "Epoch 38/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4858 - accuracy: 0.8439 - val_loss: 0.5765 - val_accuracy: 0.8208\n",
      "Epoch 39/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4796 - accuracy: 0.8468 - val_loss: 0.5643 - val_accuracy: 0.8294\n",
      "Epoch 40/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4804 - accuracy: 0.8451 - val_loss: 0.5456 - val_accuracy: 0.8332\n",
      "Epoch 41/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4814 - accuracy: 0.8449 - val_loss: 0.5709 - val_accuracy: 0.8214\n",
      "Epoch 42/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4501 - accuracy: 0.8556 - val_loss: 0.5640 - val_accuracy: 0.8324\n",
      "Epoch 43/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4503 - accuracy: 0.8564 - val_loss: 0.5323 - val_accuracy: 0.8336\n",
      "Epoch 44/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4474 - accuracy: 0.8563 - val_loss: 0.5776 - val_accuracy: 0.8294\n",
      "Epoch 45/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4452 - accuracy: 0.8584 - val_loss: 0.5106 - val_accuracy: 0.8436\n",
      "Epoch 46/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4276 - accuracy: 0.8627 - val_loss: 0.5428 - val_accuracy: 0.8354\n",
      "Epoch 47/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4134 - accuracy: 0.8693 - val_loss: 0.5497 - val_accuracy: 0.8366\n",
      "Epoch 48/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4287 - accuracy: 0.8605 - val_loss: 0.5178 - val_accuracy: 0.8434\n",
      "Epoch 49/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4229 - accuracy: 0.8619 - val_loss: 0.4942 - val_accuracy: 0.8514\n",
      "Epoch 50/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4072 - accuracy: 0.8701 - val_loss: 0.5452 - val_accuracy: 0.8360\n",
      "Epoch 51/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3957 - accuracy: 0.8731 - val_loss: 0.4849 - val_accuracy: 0.8510\n",
      "Epoch 52/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3948 - accuracy: 0.8708 - val_loss: 0.4993 - val_accuracy: 0.8496\n",
      "Epoch 53/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3864 - accuracy: 0.8727 - val_loss: 0.4717 - val_accuracy: 0.8618\n",
      "Epoch 54/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3875 - accuracy: 0.8770 - val_loss: 0.4973 - val_accuracy: 0.8456\n",
      "Epoch 55/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3707 - accuracy: 0.8818 - val_loss: 0.5150 - val_accuracy: 0.8484\n",
      "Epoch 56/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3815 - accuracy: 0.8779 - val_loss: 0.5068 - val_accuracy: 0.8464\n",
      "Epoch 57/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3715 - accuracy: 0.8805 - val_loss: 0.4649 - val_accuracy: 0.8550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3632 - accuracy: 0.8817 - val_loss: 0.4690 - val_accuracy: 0.8616\n",
      "Epoch 59/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3524 - accuracy: 0.8863 - val_loss: 0.4599 - val_accuracy: 0.8632\n",
      "Epoch 60/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3645 - accuracy: 0.8819 - val_loss: 0.4473 - val_accuracy: 0.8628\n",
      "Epoch 61/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3507 - accuracy: 0.8861 - val_loss: 0.4240 - val_accuracy: 0.8714\n",
      "Epoch 62/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3468 - accuracy: 0.8885 - val_loss: 0.4388 - val_accuracy: 0.8632\n",
      "Epoch 63/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3452 - accuracy: 0.8853 - val_loss: 0.4359 - val_accuracy: 0.8674\n",
      "Epoch 64/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3455 - accuracy: 0.8880 - val_loss: 0.4243 - val_accuracy: 0.8692\n",
      "Epoch 65/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3321 - accuracy: 0.8937 - val_loss: 0.4102 - val_accuracy: 0.8736\n",
      "Epoch 66/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3292 - accuracy: 0.8916 - val_loss: 0.4752 - val_accuracy: 0.8590\n",
      "Epoch 67/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3408 - accuracy: 0.8898 - val_loss: 0.4318 - val_accuracy: 0.8688\n",
      "Epoch 68/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3178 - accuracy: 0.8953 - val_loss: 0.4388 - val_accuracy: 0.8666\n",
      "Epoch 69/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3273 - accuracy: 0.8941 - val_loss: 0.4059 - val_accuracy: 0.8788\n",
      "Epoch 70/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3160 - accuracy: 0.8983 - val_loss: 0.5070 - val_accuracy: 0.8524\n",
      "Epoch 71/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3147 - accuracy: 0.8976 - val_loss: 0.4220 - val_accuracy: 0.8716\n",
      "Epoch 72/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3085 - accuracy: 0.8999 - val_loss: 0.4626 - val_accuracy: 0.8600\n",
      "Epoch 73/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3078 - accuracy: 0.9002 - val_loss: 0.4214 - val_accuracy: 0.8726\n",
      "Epoch 74/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3160 - accuracy: 0.8978 - val_loss: 0.4392 - val_accuracy: 0.8770\n",
      "Epoch 75/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3052 - accuracy: 0.9006 - val_loss: 0.4506 - val_accuracy: 0.8654\n",
      "Epoch 76/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3059 - accuracy: 0.9009 - val_loss: 0.4539 - val_accuracy: 0.8692\n",
      "Epoch 77/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3007 - accuracy: 0.9008 - val_loss: 0.4161 - val_accuracy: 0.8748\n",
      "Epoch 78/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2984 - accuracy: 0.9032 - val_loss: 0.4011 - val_accuracy: 0.8794\n",
      "Epoch 79/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2922 - accuracy: 0.9035 - val_loss: 0.4457 - val_accuracy: 0.8750\n",
      "Epoch 80/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2913 - accuracy: 0.9051 - val_loss: 0.4058 - val_accuracy: 0.8850\n",
      "Epoch 81/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2910 - accuracy: 0.9071 - val_loss: 0.3717 - val_accuracy: 0.8880\n",
      "Epoch 82/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2894 - accuracy: 0.9043 - val_loss: 0.4491 - val_accuracy: 0.8716\n",
      "Epoch 83/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2840 - accuracy: 0.9062 - val_loss: 0.3749 - val_accuracy: 0.8876\n",
      "Epoch 84/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2765 - accuracy: 0.9098 - val_loss: 0.3952 - val_accuracy: 0.8840\n",
      "Epoch 85/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2834 - accuracy: 0.9085 - val_loss: 0.4074 - val_accuracy: 0.8832\n",
      "Epoch 86/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2782 - accuracy: 0.9107 - val_loss: 0.4026 - val_accuracy: 0.8828\n",
      "Epoch 87/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2828 - accuracy: 0.9067 - val_loss: 0.4302 - val_accuracy: 0.8790\n",
      "Epoch 88/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2682 - accuracy: 0.9134 - val_loss: 0.3835 - val_accuracy: 0.8852\n",
      "Epoch 89/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2775 - accuracy: 0.9101 - val_loss: 0.4778 - val_accuracy: 0.8520\n",
      "Epoch 90/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2649 - accuracy: 0.9115 - val_loss: 0.3776 - val_accuracy: 0.8890\n",
      "Epoch 91/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2735 - accuracy: 0.9107 - val_loss: 0.3936 - val_accuracy: 0.8842\n",
      "Epoch 92/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2640 - accuracy: 0.9143 - val_loss: 0.4800 - val_accuracy: 0.8606\n",
      "Epoch 93/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2743 - accuracy: 0.9118 - val_loss: 0.4322 - val_accuracy: 0.8730\n",
      "Epoch 94/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2624 - accuracy: 0.9142 - val_loss: 0.4146 - val_accuracy: 0.8778\n",
      "Epoch 95/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2571 - accuracy: 0.9161 - val_loss: 0.4337 - val_accuracy: 0.8734\n",
      "Epoch 96/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2537 - accuracy: 0.9155 - val_loss: 0.3934 - val_accuracy: 0.8894\n",
      "Epoch 97/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2625 - accuracy: 0.9141 - val_loss: 0.3874 - val_accuracy: 0.8864\n",
      "Epoch 98/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2493 - accuracy: 0.9179 - val_loss: 0.4028 - val_accuracy: 0.8860\n",
      "Epoch 99/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2614 - accuracy: 0.9177 - val_loss: 0.4450 - val_accuracy: 0.8728\n",
      "Epoch 100/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2475 - accuracy: 0.9198 - val_loss: 0.3516 - val_accuracy: 0.8970\n",
      "Epoch 101/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2359 - accuracy: 0.9238 - val_loss: 0.4593 - val_accuracy: 0.8764\n",
      "Epoch 102/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2406 - accuracy: 0.9178 - val_loss: 0.4219 - val_accuracy: 0.8808\n",
      "Epoch 103/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2517 - accuracy: 0.9153 - val_loss: 0.4421 - val_accuracy: 0.8754\n",
      "Epoch 104/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2459 - accuracy: 0.9196 - val_loss: 0.3903 - val_accuracy: 0.8876\n",
      "Epoch 105/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2422 - accuracy: 0.9227 - val_loss: 0.3962 - val_accuracy: 0.8888\n",
      "Epoch 106/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2507 - accuracy: 0.9164 - val_loss: 0.3890 - val_accuracy: 0.8910\n",
      "Epoch 107/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2491 - accuracy: 0.9183 - val_loss: 0.3627 - val_accuracy: 0.8926\n",
      "Epoch 108/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2375 - accuracy: 0.9219 - val_loss: 0.3910 - val_accuracy: 0.8832\n",
      "Epoch 109/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2396 - accuracy: 0.9193 - val_loss: 0.3971 - val_accuracy: 0.8894\n",
      "Epoch 110/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2291 - accuracy: 0.9217 - val_loss: 0.3952 - val_accuracy: 0.8856\n",
      "Epoch 111/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2386 - accuracy: 0.9199 - val_loss: 0.4332 - val_accuracy: 0.8792\n",
      "Epoch 112/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2416 - accuracy: 0.9179 - val_loss: 0.3542 - val_accuracy: 0.8954\n",
      "Epoch 113/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2354 - accuracy: 0.9227 - val_loss: 0.4022 - val_accuracy: 0.8860\n",
      "Epoch 114/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2277 - accuracy: 0.9232 - val_loss: 0.4109 - val_accuracy: 0.8898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2271 - accuracy: 0.9253 - val_loss: 0.4097 - val_accuracy: 0.8824\n",
      "Epoch 116/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2265 - accuracy: 0.9239 - val_loss: 0.3981 - val_accuracy: 0.8930\n",
      "Epoch 117/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2245 - accuracy: 0.9241 - val_loss: 0.3985 - val_accuracy: 0.8850\n",
      "Epoch 118/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2352 - accuracy: 0.9208 - val_loss: 0.4463 - val_accuracy: 0.8822\n",
      "Epoch 119/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2220 - accuracy: 0.9267 - val_loss: 0.4623 - val_accuracy: 0.8776\n",
      "Epoch 120/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2226 - accuracy: 0.9286 - val_loss: 0.3814 - val_accuracy: 0.8942\n",
      "Epoch 121/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2244 - accuracy: 0.9266 - val_loss: 0.3618 - val_accuracy: 0.8964\n",
      "Epoch 122/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2166 - accuracy: 0.9281 - val_loss: 0.3656 - val_accuracy: 0.8944\n",
      "Epoch 123/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2167 - accuracy: 0.9278 - val_loss: 0.4232 - val_accuracy: 0.8816\n",
      "Epoch 124/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2356 - accuracy: 0.9203 - val_loss: 0.3821 - val_accuracy: 0.8940\n",
      "Epoch 125/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2186 - accuracy: 0.9237 - val_loss: 0.3675 - val_accuracy: 0.8944\n",
      "Epoch 126/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2116 - accuracy: 0.9288 - val_loss: 0.3529 - val_accuracy: 0.8980\n",
      "Epoch 127/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2190 - accuracy: 0.9282 - val_loss: 0.3457 - val_accuracy: 0.9018\n",
      "Epoch 128/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2175 - accuracy: 0.9281 - val_loss: 0.3607 - val_accuracy: 0.8992\n",
      "Epoch 129/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2089 - accuracy: 0.9309 - val_loss: 0.4053 - val_accuracy: 0.8848\n",
      "Epoch 130/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2087 - accuracy: 0.9307 - val_loss: 0.3656 - val_accuracy: 0.8980\n",
      "Epoch 131/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2197 - accuracy: 0.9287 - val_loss: 0.4081 - val_accuracy: 0.8872\n",
      "Epoch 132/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2088 - accuracy: 0.9329 - val_loss: 0.4618 - val_accuracy: 0.8800\n",
      "Epoch 133/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2223 - accuracy: 0.9261 - val_loss: 0.4224 - val_accuracy: 0.8892\n",
      "Epoch 134/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2059 - accuracy: 0.9325 - val_loss: 0.3988 - val_accuracy: 0.8898\n",
      "Epoch 135/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2166 - accuracy: 0.9301 - val_loss: 0.3432 - val_accuracy: 0.9042\n",
      "Epoch 136/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2033 - accuracy: 0.9338 - val_loss: 0.5080 - val_accuracy: 0.8740\n",
      "Epoch 137/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2219 - accuracy: 0.9275 - val_loss: 0.4264 - val_accuracy: 0.8798\n",
      "Epoch 138/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1930 - accuracy: 0.9383 - val_loss: 0.3942 - val_accuracy: 0.8960\n",
      "Epoch 139/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2062 - accuracy: 0.9335 - val_loss: 0.4506 - val_accuracy: 0.8810\n",
      "Epoch 140/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2081 - accuracy: 0.9328 - val_loss: 0.3903 - val_accuracy: 0.8924\n",
      "Epoch 141/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1998 - accuracy: 0.9333 - val_loss: 0.3949 - val_accuracy: 0.8962\n",
      "Epoch 142/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2023 - accuracy: 0.9333 - val_loss: 0.4572 - val_accuracy: 0.8796\n",
      "Epoch 143/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1994 - accuracy: 0.9338 - val_loss: 0.3703 - val_accuracy: 0.8972\n",
      "Epoch 144/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2148 - accuracy: 0.9329 - val_loss: 0.4027 - val_accuracy: 0.8906\n",
      "Epoch 145/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2040 - accuracy: 0.9339 - val_loss: 0.3814 - val_accuracy: 0.8936\n",
      "Epoch 146/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2173 - accuracy: 0.9281 - val_loss: 0.3903 - val_accuracy: 0.8910\n",
      "Epoch 147/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1897 - accuracy: 0.9349 - val_loss: 0.3495 - val_accuracy: 0.9020\n",
      "Epoch 148/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1952 - accuracy: 0.9362 - val_loss: 0.3731 - val_accuracy: 0.9002\n",
      "Epoch 149/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2055 - accuracy: 0.9344 - val_loss: 0.3893 - val_accuracy: 0.8912\n",
      "Epoch 150/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1995 - accuracy: 0.9335 - val_loss: 0.4247 - val_accuracy: 0.8894\n",
      "Epoch 151/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2028 - accuracy: 0.9341 - val_loss: 0.3553 - val_accuracy: 0.9036\n",
      "Epoch 152/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1865 - accuracy: 0.9379 - val_loss: 0.3700 - val_accuracy: 0.9014\n",
      "Epoch 153/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1850 - accuracy: 0.9396 - val_loss: 0.3622 - val_accuracy: 0.9034\n",
      "Epoch 154/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2109 - accuracy: 0.9311 - val_loss: 0.3852 - val_accuracy: 0.8982\n",
      "Epoch 155/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2065 - accuracy: 0.9312 - val_loss: 0.4184 - val_accuracy: 0.8936\n",
      "Epoch 156/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2040 - accuracy: 0.9343 - val_loss: 0.3690 - val_accuracy: 0.9060\n",
      "Epoch 157/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1858 - accuracy: 0.9376 - val_loss: 0.3541 - val_accuracy: 0.9056\n",
      "Epoch 158/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1911 - accuracy: 0.9375 - val_loss: 0.3652 - val_accuracy: 0.9014\n",
      "Epoch 159/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1833 - accuracy: 0.9389 - val_loss: 0.4245 - val_accuracy: 0.8894\n",
      "Epoch 160/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1751 - accuracy: 0.9413 - val_loss: 0.3842 - val_accuracy: 0.8990\n",
      "Epoch 161/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1870 - accuracy: 0.9403 - val_loss: 0.3851 - val_accuracy: 0.8924\n",
      "Epoch 162/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1817 - accuracy: 0.9395 - val_loss: 0.3961 - val_accuracy: 0.8942\n",
      "Epoch 163/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2110 - accuracy: 0.9306 - val_loss: 0.3786 - val_accuracy: 0.9002\n",
      "Epoch 164/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1853 - accuracy: 0.9371 - val_loss: 0.3735 - val_accuracy: 0.8996\n",
      "Epoch 165/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1762 - accuracy: 0.9424 - val_loss: 0.3778 - val_accuracy: 0.8984\n",
      "Epoch 166/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1780 - accuracy: 0.9402 - val_loss: 0.3844 - val_accuracy: 0.8992\n",
      "Epoch 167/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1764 - accuracy: 0.9396 - val_loss: 0.3772 - val_accuracy: 0.8998\n",
      "Epoch 168/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1813 - accuracy: 0.9402 - val_loss: 0.4077 - val_accuracy: 0.8940\n",
      "Epoch 169/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1700 - accuracy: 0.9431 - val_loss: 0.3537 - val_accuracy: 0.9082\n",
      "Epoch 170/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1859 - accuracy: 0.9385 - val_loss: 0.3728 - val_accuracy: 0.9004\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1757 - accuracy: 0.9427 - val_loss: 0.3698 - val_accuracy: 0.9036\n",
      "Epoch 172/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1761 - accuracy: 0.9421 - val_loss: 0.3527 - val_accuracy: 0.8994\n",
      "Epoch 173/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1793 - accuracy: 0.9396 - val_loss: 0.4268 - val_accuracy: 0.8900\n",
      "Epoch 174/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1848 - accuracy: 0.9381 - val_loss: 0.4006 - val_accuracy: 0.8968\n",
      "Epoch 175/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1861 - accuracy: 0.9391 - val_loss: 0.3878 - val_accuracy: 0.8976\n",
      "Epoch 176/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1687 - accuracy: 0.9437 - val_loss: 0.4136 - val_accuracy: 0.8922\n",
      "Epoch 177/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1683 - accuracy: 0.9442 - val_loss: 0.4706 - val_accuracy: 0.8844\n",
      "Epoch 178/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1748 - accuracy: 0.9422 - val_loss: 0.4072 - val_accuracy: 0.8940\n",
      "Epoch 179/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1681 - accuracy: 0.9447 - val_loss: 0.3427 - val_accuracy: 0.9052\n",
      "Epoch 180/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1900 - accuracy: 0.9382 - val_loss: 0.4035 - val_accuracy: 0.8994\n",
      "Epoch 181/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1707 - accuracy: 0.9432 - val_loss: 0.4224 - val_accuracy: 0.8940\n",
      "Epoch 182/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1727 - accuracy: 0.9422 - val_loss: 0.3488 - val_accuracy: 0.9046\n",
      "Epoch 183/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1664 - accuracy: 0.9447 - val_loss: 0.3614 - val_accuracy: 0.9070\n",
      "Epoch 184/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1720 - accuracy: 0.9441 - val_loss: 0.4018 - val_accuracy: 0.8992\n",
      "Epoch 185/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1761 - accuracy: 0.9411 - val_loss: 0.3999 - val_accuracy: 0.8958\n",
      "Epoch 186/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1602 - accuracy: 0.9467 - val_loss: 0.4182 - val_accuracy: 0.8912\n",
      "Epoch 187/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1673 - accuracy: 0.9452 - val_loss: 0.4242 - val_accuracy: 0.8888\n",
      "Epoch 188/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1610 - accuracy: 0.9452 - val_loss: 0.3801 - val_accuracy: 0.9028\n",
      "Epoch 189/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1696 - accuracy: 0.9435 - val_loss: 0.4826 - val_accuracy: 0.8860\n",
      "Epoch 190/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1885 - accuracy: 0.9395 - val_loss: 0.4624 - val_accuracy: 0.8860\n",
      "Epoch 191/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1781 - accuracy: 0.9414 - val_loss: 0.4027 - val_accuracy: 0.8984\n",
      "Epoch 192/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1510 - accuracy: 0.9497 - val_loss: 0.4415 - val_accuracy: 0.8892\n",
      "Epoch 193/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1653 - accuracy: 0.9451 - val_loss: 0.3576 - val_accuracy: 0.9118\n",
      "Epoch 194/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1467 - accuracy: 0.9501 - val_loss: 0.3882 - val_accuracy: 0.8990\n",
      "Epoch 195/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2078 - accuracy: 0.9338 - val_loss: 0.3548 - val_accuracy: 0.9066\n",
      "Epoch 196/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1508 - accuracy: 0.9517 - val_loss: 0.3550 - val_accuracy: 0.9088\n",
      "Epoch 197/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1541 - accuracy: 0.9462 - val_loss: 0.4094 - val_accuracy: 0.9018\n",
      "Epoch 198/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1602 - accuracy: 0.9459 - val_loss: 0.3876 - val_accuracy: 0.9050\n",
      "Epoch 199/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1616 - accuracy: 0.9475 - val_loss: 0.4604 - val_accuracy: 0.8902\n",
      "Epoch 200/200\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1729 - accuracy: 0.9413 - val_loss: 0.4024 - val_accuracy: 0.8992\n",
      "157/157 [==============================] - 0s 1ms/step - loss: 0.4024 - accuracy: 0.8992\n",
      "Loss: 0.4023872911930084\n",
      "Accuracy: 0.8992000222206116\n",
      "INFO:tensorflow:Assets written to: letter_classifier2.model\\assets\n"
     ]
    }
   ],
   "source": [
    "#Create the NN  (Dense layer and then get probabilities)\n",
    "model=models.Sequential()\n",
    "model.add(layers.Dense(16,activation='relu',input_shape = (16,)))\n",
    "model.add(layers.Dense(100,activation= 'relu'))\n",
    "model.add(layers.Dense(32,activation= 'relu'))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(32,activation= 'relu'))\n",
    "model.add(layers.Dense(8,activation= 'relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "#Finally let's get probabilities\n",
    "model.add(layers.Dense(26,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#We say how many times the NN will see the training data (the generations I supose)\n",
    "model.fit(training_images, training_labels, epochs=200, validation_data=(testing_images, testing_labels))\n",
    "\n",
    "loss, accuracy = model.evaluate(testing_images, testing_labels)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "model.save('letter_classifier2.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8212bf4d",
   "metadata": {},
   "source": [
    "# USE AN ALREADY TRAINED NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5174d221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "31\n",
      "32\n",
      "33\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "78\n",
      "79\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "100\n",
      "101\n",
      "102\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "250\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "486\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "583\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "613\n",
      "614\n",
      "615\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "888\n",
      "889\n",
      "890\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "941\n",
      "942\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1221\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1325\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1375\n",
      "1376\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1495\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1822\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1879\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2112\n",
      "2113\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2125\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2413\n",
      "2414\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "2444\n",
      "2445\n",
      "2446\n",
      "2447\n",
      "2448\n",
      "2449\n",
      "2450\n",
      "2451\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2458\n",
      "2459\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2464\n",
      "2465\n",
      "2466\n",
      "2467\n",
      "2468\n",
      "2469\n",
      "2470\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2474\n",
      "2475\n",
      "2476\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2483\n",
      "2483\n",
      "2484\n",
      "2485\n",
      "2486\n",
      "2487\n",
      "2488\n",
      "2489\n",
      "2490\n",
      "2491\n",
      "2492\n",
      "2493\n",
      "2494\n",
      "2495\n",
      "2496\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "2500\n",
      "2501\n",
      "2502\n",
      "2503\n",
      "2504\n",
      "2505\n",
      "2506\n",
      "2507\n",
      "2508\n",
      "2509\n",
      "2510\n",
      "2511\n",
      "2512\n",
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2519\n",
      "2520\n",
      "2520\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2528\n",
      "2529\n",
      "2530\n",
      "2531\n",
      "2532\n",
      "2533\n",
      "2534\n",
      "2535\n",
      "2536\n",
      "2537\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2541\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "2551\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2559\n",
      "2560\n",
      "2561\n",
      "2562\n",
      "2563\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2567\n",
      "2568\n",
      "2569\n",
      "2570\n",
      "2571\n",
      "2572\n",
      "2573\n",
      "2574\n",
      "2575\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2581\n",
      "2582\n",
      "2583\n",
      "2584\n",
      "2585\n",
      "2586\n",
      "2587\n",
      "2588\n",
      "2589\n",
      "2590\n",
      "2591\n",
      "2592\n",
      "2593\n",
      "2593\n",
      "2594\n",
      "2595\n",
      "2596\n",
      "2597\n",
      "2598\n",
      "2599\n",
      "2600\n",
      "2600\n",
      "2601\n",
      "2602\n",
      "2603\n",
      "2604\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2608\n",
      "2609\n",
      "2610\n",
      "2611\n",
      "2612\n",
      "2613\n",
      "2614\n",
      "2615\n",
      "2616\n",
      "2617\n",
      "2618\n",
      "2618\n",
      "2619\n",
      "2620\n",
      "2621\n",
      "2622\n",
      "2623\n",
      "2623\n",
      "2624\n",
      "2625\n",
      "2626\n",
      "2627\n",
      "2628\n",
      "2629\n",
      "2630\n",
      "2631\n",
      "2632\n",
      "2633\n",
      "2634\n",
      "2634\n",
      "2635\n",
      "2636\n",
      "2637\n",
      "2637\n",
      "2638\n",
      "2639\n",
      "2640\n",
      "2641\n",
      "2642\n",
      "2643\n",
      "2644\n",
      "2644\n",
      "2645\n",
      "2646\n",
      "2647\n",
      "2648\n",
      "2649\n",
      "2650\n",
      "2651\n",
      "2652\n",
      "2653\n",
      "2654\n",
      "2655\n",
      "2656\n",
      "2657\n",
      "2658\n",
      "2659\n",
      "2660\n",
      "2661\n",
      "2662\n",
      "2663\n",
      "2664\n",
      "2665\n",
      "2666\n",
      "2667\n",
      "2668\n",
      "2669\n",
      "2669\n",
      "2670\n",
      "2670\n",
      "2671\n",
      "2671\n",
      "2672\n",
      "2672\n",
      "2673\n",
      "2674\n",
      "2675\n",
      "2676\n",
      "2677\n",
      "2677\n",
      "2677\n",
      "2678\n",
      "2679\n",
      "2680\n",
      "2681\n",
      "2682\n",
      "2683\n",
      "2684\n",
      "2685\n",
      "2686\n",
      "2687\n",
      "2688\n",
      "2689\n",
      "2690\n",
      "2691\n",
      "2692\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2696\n",
      "2697\n",
      "2698\n",
      "2699\n",
      "2700\n",
      "2701\n",
      "2702\n",
      "2703\n",
      "2704\n",
      "2705\n",
      "2706\n",
      "2707\n",
      "2708\n",
      "2709\n",
      "2710\n",
      "2711\n",
      "2712\n",
      "2713\n",
      "2714\n",
      "2715\n",
      "2716\n",
      "2717\n",
      "2718\n",
      "2719\n",
      "2719\n",
      "2720\n",
      "2721\n",
      "2722\n",
      "2723\n",
      "2724\n",
      "2725\n",
      "2726\n",
      "2727\n",
      "2728\n",
      "2729\n",
      "2730\n",
      "2731\n",
      "2732\n",
      "2733\n",
      "2734\n",
      "2735\n",
      "2736\n",
      "2737\n",
      "2738\n",
      "2739\n",
      "2740\n",
      "2740\n",
      "2741\n",
      "2742\n",
      "2743\n",
      "2744\n",
      "2745\n",
      "2745\n",
      "2746\n",
      "2746\n",
      "2747\n",
      "2747\n",
      "2748\n",
      "2749\n",
      "2750\n",
      "2751\n",
      "2751\n",
      "2752\n",
      "2753\n",
      "2754\n",
      "2755\n",
      "2756\n",
      "2757\n",
      "2758\n",
      "2759\n",
      "2760\n",
      "2761\n",
      "2762\n",
      "2763\n",
      "2764\n",
      "2764\n",
      "2765\n",
      "2766\n",
      "2767\n",
      "2768\n",
      "2769\n",
      "2770\n",
      "2771\n",
      "2772\n",
      "2773\n",
      "2774\n",
      "2775\n",
      "2776\n",
      "2777\n",
      "2777\n",
      "2778\n",
      "2778\n",
      "2779\n",
      "2780\n",
      "2781\n",
      "2782\n",
      "2783\n",
      "2784\n",
      "2785\n",
      "2786\n",
      "2787\n"
     ]
    }
   ],
   "source": [
    "Itry=Ntraining\n",
    "counter= 0\n",
    "\n",
    "#Give names to the different types 1, type 2 ... type 9\n",
    "class_names = ['A', 'B', 'C', 'D', 'E', 'D', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'X', 'Y', 'Z']\n",
    " \n",
    "model = models.load_model('letter_classifier3.model')\n",
    "\n",
    "#Try the NN for the testing values\n",
    "for i in range(20000-Ntraining):\n",
    "    prediction= model.predict(df.drop(columns='class').iloc[[Itry+i]])\n",
    "    index=np.argmax(prediction)\n",
    "    \n",
    "    if (index==int(df['class'].iloc[[Itry+i]])):\n",
    "        counter=counter + 1\n",
    "    print(counter)\n",
    "#Print all the cases, with the real value and the predicted value\n",
    "    #print(f'Prediction is {class_names[index]}')\n",
    "    #print('The real value is' , class_names[int(df['class'].iloc[[Itry+i]])])\n",
    "    #print()\n",
    "\n",
    "#Get the accuracy over the testing sample\n",
    "print(counter/5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dae127",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad675ba",
   "metadata": {},
   "source": [
    " # IMPORT DATASET 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6407bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = arff.loadarff('letter-recognition.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "Ntraining=15000 #Total is 20k\n",
    "\n",
    "training_images_df=df.drop(columns='class')[:Ntraining]\n",
    "testing_images_df=df.drop(columns='class')[Ntraining:]\n",
    "\n",
    "training_labels_df=df['class'][:Ntraining]\n",
    "testing_labels_df=df['class'][Ntraining:]\n",
    "\n",
    "training_images_df=training_images_df.to_numpy()\n",
    "training_labels_df=training_labels_df.to_numpy(dtype='str')\n",
    "testing_images_df=testing_images_df.to_numpy()\n",
    "testing_labels_df=testing_labels_df.to_numpy(dtype='str')\n",
    "\n",
    "print(testing_images_df)\n",
    "\n",
    "#training_images=[]\n",
    "training_labels=[]\n",
    "#testing_images=[]\n",
    "testing_labels=[]\n",
    "\n",
    "for i in range(len(training_labels_df)):\n",
    "    #training_images.append(training_images_df[i])\n",
    "    training_labels.append(training_labels_df[i])\n",
    "for i in range(len(testing_labels_df)):\n",
    "    #testing_images.append(testing_images_df[i])\n",
    "    testing_labels.append(testing_labels_df[i])\n",
    "    \n",
    "for i in range (len(training_labels)):\n",
    "    training_labels[i]=ord(training_labels[i])\n",
    "    \n",
    "for i in range (len(testing_labels)):\n",
    "    testing_labels[i]=ord(testing_labels[i])\n",
    "\n",
    "training_lables=np.array(training_labels)\n",
    "testing_labels=np.array(testing_labels)\n",
    "\n",
    "training_labels=np.reshape((2,2))se(training_labels)\n",
    "testing_labels=np.transpose(testing_labels)\n",
    "\n",
    "\n",
    "print(testing_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
